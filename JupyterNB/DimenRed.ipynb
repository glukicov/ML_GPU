{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../helpers/header.py\n",
    "SUB_DIR = \"DimenRed\"\n",
    "# load ENV path to project from .profile \n",
    "import os, sys\n",
    "PROJECT_ROOT_DIR=os.environ.get('ML_PATH')\n",
    "sys.path.append(os.path.join(PROJECT_ROOT_DIR, \"helpers\")) # add helper modules to path\n",
    "\n",
    "# MPL \n",
    "import MPL_header #load common MPL imports (from helpers)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# %matplotlib widget \n",
    "\n",
    "# NP, constant seed, PD \n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "import pandas as pd\n",
    "\n",
    "# Where to save the figures\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"fig\", SUB_DIR)\n",
    "# IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"fig\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The more dimensions the training set has, the greater the risk of overfitting it. One solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Two main approaches to reducing dimensionality: projection and Manifold Learning. Training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. In general/often, the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. Reducing the dimensionality of your training set before training a model will usually speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: select hyperplane with largest variance (more info and minmises MSD to projection)\n",
    "## Finds principal axis of max variance, and the next axis (up to D) - ortogonal to previous - to account for remaning variacne in other D.\n",
    "## !N.B. PCA (outside of sklearn) needs centered data as input for the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from sklearn.decomposition import PCA`  \n",
    "`pca = PCA(n_components = 2)`    \n",
    "`X2D = pca.fit_transform(X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pca.explained_variance_ratio_` allows to check variance in each D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing D to explain e.g. 95% of variance\n",
    "`pca = PCA()`  \n",
    "`pca.fit(X_train)`  \n",
    "`cumsum = np.cumsum(pca.explained_variance_ratio_)`  \n",
    "`d = np.argmax(cumsum >= 0.95) + 1`  \n",
    "## or simply\n",
    "`pca = PCA(n_components=0.95)`  \n",
    "`X_reduced = pca.fit_transform(X_train)`  \n",
    "\n",
    "## also nice to plot cumsum vs. d - looking for a shoulder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruction error = mean squared distance between the original data and the reconstructed data\n",
    "`pca = PCA(n_components = 154)`  \n",
    "`X_reduced = pca.fit_transform(X_train)`  \n",
    "`X_recovered = pca.inverse_transform(X_reduced)`  \n",
    "`$X_{rec} = X_{d-proj}W_{d}^T$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA: one mini-batch at a time, for large datasets that can't fit into memory `from sklearn.decomposition import IncrementalPCA`, or `np.memmap` to manipulate a large array stored in a binary file on disk as if it were entirely in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N.B. kernel trick: a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from sklearn.decomposition import KernelPCA`  \n",
    "`rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)`  \n",
    "`X_reduced = rbf_pca.fit_transform(X)`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you can use grid search to select the kernel and hyperparameters that lead to the best performance on a task (e.g. classification) (unsup-supervised method)\n",
    "`from sklearn.model_selection import GridSearchCV `  \n",
    "`from sklearn.linear_model import LogisticRegression `  \n",
    "`from sklearn.pipeline import Pipeline`  \n",
    "`clf = Pipeline([  (\"kpca\", KernelPCA(n_components=2)),  (\"log_reg\", LogisticRegression()) ])`  \n",
    "`param_grid = [{ \"kpca__gamma\": np.linspace(0.03, 0.05, 10), \"kpca__kernel\": [\"rbf\", \"sigmoid\"] }]`  \n",
    "`grid_search = GridSearchCV(clf, param_grid, cv=3)`   \n",
    "`grid_search.fit(X, y)`  \n",
    "` print(grid_search.best_params_)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## or entirely unsupervsied: lowest reconstruction error. N.B. see finder point on true vs pre-image errror! `fit_inverse_transform=True` and `inverse_transform()` for KernelPCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Linear Embedding (LLE) - preserving closest neighbours' relations\n",
    "##  is another powerful nonlinear dimensionality reduc‚Äê tion (NLDR) technique. It is a Manifold Learning technique that does not rely on projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from sklearn.manifold import LocallyLinearEmbedding`  \n",
    "`lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)`  \n",
    "`X_reduced = lle.fit_transform(X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other options: Random Projections, Multidimensional Scaling (MDS), Isomap, t-SNE...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}